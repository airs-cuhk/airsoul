---
run_name: [RUN_NAME]
master_port: "12402"
load_model_path: [Path or None]

log_config:
  use_tensorboard: True
  tensorboard_log: [Path]
  training_log: [Path]
  evaluation_log: [Path]

model_config:
  context_warmup: 512
  action_dim: 236
  max_position_loss_weighting: 128000
  policy_loss_type: CrossEntropy

  vocab_size: 236 # nobs + nagent + ntag + value_num + 10
  nobs: 10
  nagent: 10
  ntag: 6
  default_tag: 5
  value_num: 200
  resolution: 0.1

  word_embeddings:
      model_type: MLP
      input_type: Discrete
      input_size: 236
      hidden_size: 1024
      dropout: 0.0
  output_layers:
      model_type: MLP
      output_type: Discrete
      input_size: 1024
      hidden_size: 236
      layer_norm: True
      residual_connect: False
      dropout: 0.0

  causal_block_bak_1: # change to causal_block if use this block
      model_type: TRANSFORMER
      num_layers: 6
      hidden_size: 1024
      nhead: 8
      inner_hidden_size: 2048
      dropout: 0.10
      context_window: -1
      checkpoints_density: -1
      position_encoding_size: 12000
      use_layer_norm: True
      use_blockrecurrence: True
      memory_length: 3000
      memory_type: KV
      is_frozen: False

  causal_block_bak_2:
      model_type: GSA # GLA
      num_layers: 18
      hidden_size: 1024
      inner_hidden_size: 2048
      dropout: 0.10
      nhead: 8
      use_layer_norm: True
      gate_bound: 22
      use_blockrecurrence: True
      checkpoints_density: -1
      memory_type: MEM
      memory_length: 0 
      is_frozen: False
      is_generate: False

  causal_block_bak_3:
      model_type: RWKV6
      num_layers: 12
      hidden_size: 1024
      inner_hidden_size: 2048
      dropout: 0.10
      nhead: 16
      expand_k: 1
      expand_v: 2
      hidden_ratio: 3.5
      use_layer_norm: True
      use_blockrecurrence: True
      checkpoints_density: -1
      gate_bound: 12
      memory_type: MEM
      memory_length: 0
      is_frozen: False

  causal_block:
      model_type: RWKV7
      num_layers: 12
      hidden_size: 1024
      inner_hidden_size: 2048
      dropout: 0.10
      nhead: 8
      use_layer_norm: True
      use_blockrecurrence: True
      checkpoints_density: -1
      memory_type: MEM
      memory_length: 0
      is_frozen: False

  causal_block_bak_5:
      model_type: DELTANET
      num_layers: 18
      hidden_size: 1024
      inner_hidden_size: 2048
      dropout: 0.10
      nhead: 8
      expand_v: 1 # default 2
      use_layer_norm: True
      use_blockrecurrence: True
      checkpoints_density: -1
      memory_type: MEM
      memory_length: 0
      is_frozen: False
      is_generate: False

  causal_block_bak_6:
      model_type: Mamba
      num_layers: 12
      hidden_size: 1024
      inner_hidden_size: 2048
      expand: 2
      d_conv: 4
      d_state: 16
      dropout: 0.10
      position_encoding_size: 12000
      use_layer_norm: True
      use_blockrecurrence: True
      checkpoints_density: -1
      memory_length: 0
      memory_type: MEM
      is_frozen: False

  causal_block_bak_7:
      model_type: Mamba2
      num_layers: 12
      hidden_size: 1024
      inner_hidden_size: 2048
      dropout: 0.10
      nhead: 8
      use_segment_input: True
      use_layer_norm: True
      use_blockrecurrence: True
      checkpoints_density: -1
      memory_length: 0
      memory_type: MEM
      is_frozen: False

train_config:
    max_epochs: 20
    batch_size: 12

    seq_len: 16000
    seg_len: 4000

    manual_sync: True

    lr: 2.0e-4
    lr_decay_interval: 2000
    lr_start_step: 8000

    data_path: [Path]
    save_model_path: [Path]
    max_save_iterations: 1000

    state_dropout: 0.0
    reward_dropout: 0.0

    lossweight_policymodel: 0.75
    lossweight_worldmodel_states: 0.15
    lossweight_worldmodel_actions: 0.05
    lossweight_worldmodel_rewards: 0.05
    lossweight_entropy: 0.0
    lossweight_l2: 0.0

    use_amp: False
    use_scaler: False

test_config:
    batch_size: 16
    data_path: [Path]
    output: "./offline_eval/"
    seq_len: 16000
    seg_len: 4000

generator_config:
    env: hvac
    task_file: [PATH]
    epoch_numbers: 1
    downsample_trail: 30
    decoding_strategy:
        T_ini: 1.0
        T_fin: 0.1
        T_step: 10000
        decay_type: Linear
    max_trails: 1000 
    max_steps: 200
    max_total_steps: 0
    learn_from_data: False
    data_root: [Path]
    output: "./online_eval/"
    vocab:
        max_obs_num: 10
        max_agent_num: 10
        tag_num: 6
        value_num: 300
        resolution: 0.1
        vocab_size: 236